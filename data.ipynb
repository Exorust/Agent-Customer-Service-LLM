{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "dataset = load_dataset(\"datasets/NebulaByte/E-Commerce_Customer_Support_Conversations\")\n",
    "# https://huggingface.co/datasets/NebulaByte/E-Commerce_FAQs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import streamlit as st\n",
    "from langchain.output_parsers import PydanticOutputParser\n",
    "from openai import OpenAI\n",
    "import os\n",
    "from typing import List\n",
    "\n",
    "import streamlit as st\n",
    "from langchain.llms import OpenAI\n",
    "\n",
    "OPEN_AI_temp = 0.7\n",
    "\n",
    "st.title('Service LLM Chat')\n",
    "\n",
    "openai_api_key = st.sidebar.text_input('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', type='password')\n",
    "\n",
    "def generate_response(input_text):\n",
    "    llm = OpenAI(temperature=OPEN_AI_temp, openai_api_key=openai_api_key)\n",
    "    st.info(llm(input_text))\n",
    "\n",
    "with st.form('my_form'):\n",
    "    text = st.text_area('Enter text:', 'What is the issue, could you please explain in detail so I could help you?')\n",
    "    submitted = st.form_submit_button('Submit')\n",
    "    if not openai_api_key.startswith('sk-'):\n",
    "        st.warning('Please enter your OpenAI API key!', icon='⚠')\n",
    "    if submitted and openai_api_key.startswith('sk-'):\n",
    "        generate_response(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import streamlit as st\n",
    "from langchain.llms import OpenAI\n",
    "from langchain.reacts import ReAct\n",
    "\n",
    "st.title('Service LLM Chat')\n",
    "openai_api_key = st.sidebar.text_input('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', type='password')\n",
    "OPEN_AI_temp = 0.7\n",
    "\n",
    "def recursive_tree_logic(input_text, context):\n",
    "    pass\n",
    "\n",
    "def generate_response(input_text, context):\n",
    "    llm = OpenAI(temperature=OPEN_AI_temp, openai_api_key=openai_api_key)\n",
    "    response = llm(input_text)  # Get the initial response from the LLM\n",
    "\n",
    "    next_step = recursive_tree_logic(response, context)\n",
    "\n",
    "    return next_step\n",
    "\n",
    "with st.form('my_form'):\n",
    "    text = st.text_area('Enter text:', 'How can I assist you today?')\n",
    "    submitted = st.form_submit_button('Submit')\n",
    "\n",
    "    if submitted and openai_api_key.startswith('sk-'):\n",
    "        context = {}  # Initialize or update your context here\n",
    "        response = generate_response(text, context)\n",
    "        st.write(response)\n",
    "    elif not openai_api_key.startswith('sk-'):\n",
    "        st.warning('Please enter a valid OpenAI API key!', icon='⚠')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting langchain\n",
      "  Downloading langchain-0.1.0-py3-none-any.whl (797 kB)\n",
      "     -------------------------------------- 798.0/798.0 kB 7.2 MB/s eta 0:00:00\n",
      "Collecting requests<3,>=2\n",
      "  Downloading requests-2.31.0-py3-none-any.whl (62 kB)\n",
      "     ---------------------------------------- 62.6/62.6 kB 3.3 MB/s eta 0:00:00\n",
      "Collecting SQLAlchemy<3,>=1.4\n",
      "  Downloading SQLAlchemy-2.0.25-cp38-cp38-win_amd64.whl (2.1 MB)\n",
      "     ---------------------------------------- 2.1/2.1 MB 26.4 MB/s eta 0:00:00\n",
      "Collecting aiohttp<4.0.0,>=3.8.3\n",
      "  Downloading aiohttp-3.9.1-cp38-cp38-win_amd64.whl (367 kB)\n",
      "     ------------------------------------- 367.1/367.1 kB 22.3 MB/s eta 0:00:00\n",
      "Collecting tenacity<9.0.0,>=8.1.0\n",
      "  Downloading tenacity-8.2.3-py3-none-any.whl (24 kB)\n",
      "Collecting dataclasses-json<0.7,>=0.5.7\n",
      "  Downloading dataclasses_json-0.6.3-py3-none-any.whl (28 kB)\n",
      "Collecting langsmith<0.1.0,>=0.0.77\n",
      "  Downloading langsmith-0.0.80-py3-none-any.whl (48 kB)\n",
      "     ---------------------------------------- 48.3/48.3 kB 2.4 MB/s eta 0:00:00\n",
      "Requirement already satisfied: PyYAML>=5.3 in c:\\users\\shauw\\miniconda3\\envs\\cs585\\lib\\site-packages (from langchain) (6.0)\n",
      "Collecting pydantic<3,>=1\n",
      "  Downloading pydantic-2.5.3-py3-none-any.whl (381 kB)\n",
      "     ------------------------------------ 381.9/381.9 kB 794.2 kB/s eta 0:00:00\n",
      "Requirement already satisfied: numpy<2,>=1 in c:\\users\\shauw\\miniconda3\\envs\\cs585\\lib\\site-packages (from langchain) (1.24.4)\n",
      "Collecting async-timeout<5.0.0,>=4.0.0\n",
      "  Downloading async_timeout-4.0.3-py3-none-any.whl (5.7 kB)\n",
      "Collecting jsonpatch<2.0,>=1.33\n",
      "  Downloading jsonpatch-1.33-py2.py3-none-any.whl (12 kB)\n",
      "Collecting langchain-core<0.2,>=0.1.7\n",
      "  Downloading langchain_core-0.1.10-py3-none-any.whl (216 kB)\n",
      "     ------------------------------------- 216.6/216.6 kB 13.8 MB/s eta 0:00:00\n",
      "Collecting langchain-community<0.1,>=0.0.9\n",
      "  Downloading langchain_community-0.0.12-py3-none-any.whl (1.6 MB)\n",
      "     ---------------------------------------- 1.6/1.6 MB 24.9 MB/s eta 0:00:00\n",
      "Collecting multidict<7.0,>=4.5\n",
      "  Using cached multidict-6.0.4-cp38-cp38-win_amd64.whl (28 kB)\n",
      "Collecting frozenlist>=1.1.1\n",
      "  Downloading frozenlist-1.4.1-cp38-cp38-win_amd64.whl (50 kB)\n",
      "     ---------------------------------------- 50.8/50.8 kB 2.5 MB/s eta 0:00:00\n",
      "Collecting aiosignal>=1.1.2\n",
      "  Using cached aiosignal-1.3.1-py3-none-any.whl (7.6 kB)\n",
      "Collecting yarl<2.0,>=1.0\n",
      "  Downloading yarl-1.9.4-cp38-cp38-win_amd64.whl (77 kB)\n",
      "     ---------------------------------------- 77.1/77.1 kB 4.2 MB/s eta 0:00:00\n",
      "Requirement already satisfied: attrs>=17.3.0 in c:\\users\\shauw\\miniconda3\\envs\\cs585\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (22.2.0)\n",
      "Collecting marshmallow<4.0.0,>=3.18.0\n",
      "  Downloading marshmallow-3.20.2-py3-none-any.whl (49 kB)\n",
      "     ---------------------------------------- 49.4/49.4 kB 2.6 MB/s eta 0:00:00\n",
      "Collecting typing-inspect<1,>=0.4.0\n",
      "  Downloading typing_inspect-0.9.0-py3-none-any.whl (8.8 kB)\n",
      "Requirement already satisfied: jsonpointer>=1.9 in c:\\users\\shauw\\miniconda3\\envs\\cs585\\lib\\site-packages (from jsonpatch<2.0,>=1.33->langchain) (2.3)\n",
      "Collecting packaging<24.0,>=23.2\n",
      "  Downloading packaging-23.2-py3-none-any.whl (53 kB)\n",
      "     ---------------------------------------- 53.0/53.0 kB 2.9 MB/s eta 0:00:00\n",
      "Requirement already satisfied: anyio<5,>=3 in c:\\users\\shauw\\miniconda3\\envs\\cs585\\lib\\site-packages (from langchain-core<0.2,>=0.1.7->langchain) (3.6.2)\n",
      "Collecting pydantic-core==2.14.6\n",
      "  Downloading pydantic_core-2.14.6-cp38-none-win_amd64.whl (1.9 MB)\n",
      "     ---------------------------------------- 1.9/1.9 MB 30.3 MB/s eta 0:00:00\n",
      "Collecting annotated-types>=0.4.0\n",
      "  Downloading annotated_types-0.6.0-py3-none-any.whl (12 kB)\n",
      "Collecting typing-extensions>=4.6.1\n",
      "  Downloading typing_extensions-4.9.0-py3-none-any.whl (32 kB)\n",
      "Collecting urllib3<3,>=1.21.1\n",
      "  Downloading urllib3-2.1.0-py3-none-any.whl (104 kB)\n",
      "     -------------------------------------- 104.6/104.6 kB 6.3 MB/s eta 0:00:00\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\shauw\\miniconda3\\envs\\cs585\\lib\\site-packages (from requests<3,>=2->langchain) (2023.7.22)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\shauw\\miniconda3\\envs\\cs585\\lib\\site-packages (from requests<3,>=2->langchain) (3.4)\n",
      "Collecting charset-normalizer<4,>=2\n",
      "  Downloading charset_normalizer-3.3.2-cp38-cp38-win_amd64.whl (99 kB)\n",
      "     ---------------------------------------- 99.6/99.6 kB 5.6 MB/s eta 0:00:00\n",
      "Collecting greenlet!=0.4.17\n",
      "  Downloading greenlet-3.0.3-cp38-cp38-win_amd64.whl (290 kB)\n",
      "     ------------------------------------- 290.8/290.8 kB 17.5 MB/s eta 0:00:00\n",
      "Requirement already satisfied: sniffio>=1.1 in c:\\users\\shauw\\miniconda3\\envs\\cs585\\lib\\site-packages (from anyio<5,>=3->langchain-core<0.2,>=0.1.7->langchain) (1.3.0)\n",
      "Collecting mypy-extensions>=0.3.0\n",
      "  Downloading mypy_extensions-1.0.0-py3-none-any.whl (4.7 kB)\n",
      "Installing collected packages: urllib3, typing-extensions, tenacity, packaging, mypy-extensions, multidict, jsonpatch, greenlet, frozenlist, charset-normalizer, async-timeout, yarl, typing-inspect, SQLAlchemy, requests, pydantic-core, marshmallow, annotated-types, aiosignal, pydantic, dataclasses-json, aiohttp, langsmith, langchain-core, langchain-community, langchain\n",
      "  Attempting uninstall: packaging\n",
      "    Found existing installation: packaging 23.0\n",
      "    Uninstalling packaging-23.0:\n",
      "      Successfully uninstalled packaging-23.0\n",
      "Successfully installed SQLAlchemy-2.0.25 aiohttp-3.9.1 aiosignal-1.3.1 annotated-types-0.6.0 async-timeout-4.0.3 charset-normalizer-3.3.2 dataclasses-json-0.6.3 frozenlist-1.4.1 greenlet-3.0.3 jsonpatch-1.33 langchain-0.1.0 langchain-community-0.0.12 langchain-core-0.1.10 langsmith-0.0.80 marshmallow-3.20.2 multidict-6.0.4 mypy-extensions-1.0.0 packaging-23.2 pydantic-2.5.3 pydantic-core-2.14.6 requests-2.31.0 tenacity-8.2.3 typing-extensions-4.9.0 typing-inspect-0.9.0 urllib3-2.1.0 yarl-1.9.4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "tensorflow-intel 2.12.0 requires flatbuffers>=2.0, which is not installed.\n",
      "tensorflow-intel 2.12.0 requires libclang>=13.0.0, which is not installed.\n",
      "tensorflow-intel 2.12.0 requires protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3, which is not installed.\n",
      "tensorflow-intel 2.12.0 requires tensorflow-estimator<2.13,>=2.12.0, which is not installed.\n",
      "tensorflow-intel 2.12.0 requires tensorflow-io-gcs-filesystem>=0.23.1; platform_machine != \"arm64\" or platform_system != \"Darwin\", which is not installed.\n",
      "tensorflow-intel 2.12.0 requires termcolor>=1.1.0, which is not installed.\n",
      "tensorflow-intel 2.12.0 requires wrapt<1.15,>=1.11.0, which is not installed.\n",
      "tensorboard 2.12.2 requires markdown>=2.6.8, which is not installed.\n",
      "tensorboard 2.12.2 requires protobuf>=3.19.6, which is not installed.\n",
      "tensorboard 2.12.2 requires tensorboard-data-server<0.8.0,>=0.7.0, which is not installed.\n",
      "tensorboard 2.12.2 requires tensorboard-plugin-wit>=1.6.0, which is not installed.\n",
      "tensorboard 2.12.2 requires werkzeug>=1.0.1, which is not installed.\n",
      "requests-oauthlib 1.3.1 requires oauthlib>=3.0.0, which is not installed.\n",
      "tensorflow-intel 2.12.0 requires numpy<1.24,>=1.22, but you have numpy 1.24.4 which is incompatible.\n"
     ]
    }
   ],
   "source": [
    "!pip install langchain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting langchain-openai\n",
      "  Downloading langchain_openai-0.0.2.post1-py3-none-any.whl (28 kB)\n",
      "Requirement already satisfied: numpy<2,>=1 in c:\\users\\shauw\\miniconda3\\envs\\cs585\\lib\\site-packages (from langchain-openai) (1.24.4)\n",
      "Collecting openai<2.0.0,>=1.6.1\n",
      "  Downloading openai-1.7.2-py3-none-any.whl (212 kB)\n",
      "     -------------------------------------- 212.1/212.1 kB 6.5 MB/s eta 0:00:00\n",
      "Requirement already satisfied: langchain-core<0.2,>=0.1.7 in c:\\users\\shauw\\miniconda3\\envs\\cs585\\lib\\site-packages (from langchain-openai) (0.1.10)\n",
      "Collecting tiktoken<0.6.0,>=0.5.2\n",
      "  Downloading tiktoken-0.5.2-cp38-cp38-win_amd64.whl (786 kB)\n",
      "     ------------------------------------- 786.4/786.4 kB 16.5 MB/s eta 0:00:00\n",
      "Requirement already satisfied: tenacity<9.0.0,>=8.1.0 in c:\\users\\shauw\\miniconda3\\envs\\cs585\\lib\\site-packages (from langchain-core<0.2,>=0.1.7->langchain-openai) (8.2.3)\n",
      "Requirement already satisfied: langsmith<0.1.0,>=0.0.63 in c:\\users\\shauw\\miniconda3\\envs\\cs585\\lib\\site-packages (from langchain-core<0.2,>=0.1.7->langchain-openai) (0.0.80)\n",
      "Requirement already satisfied: requests<3,>=2 in c:\\users\\shauw\\miniconda3\\envs\\cs585\\lib\\site-packages (from langchain-core<0.2,>=0.1.7->langchain-openai) (2.31.0)\n",
      "Requirement already satisfied: pydantic<3,>=1 in c:\\users\\shauw\\miniconda3\\envs\\cs585\\lib\\site-packages (from langchain-core<0.2,>=0.1.7->langchain-openai) (2.5.3)\n",
      "Requirement already satisfied: anyio<5,>=3 in c:\\users\\shauw\\miniconda3\\envs\\cs585\\lib\\site-packages (from langchain-core<0.2,>=0.1.7->langchain-openai) (3.6.2)\n",
      "Requirement already satisfied: PyYAML>=5.3 in c:\\users\\shauw\\miniconda3\\envs\\cs585\\lib\\site-packages (from langchain-core<0.2,>=0.1.7->langchain-openai) (6.0)\n",
      "Requirement already satisfied: jsonpatch<2.0,>=1.33 in c:\\users\\shauw\\miniconda3\\envs\\cs585\\lib\\site-packages (from langchain-core<0.2,>=0.1.7->langchain-openai) (1.33)\n",
      "Requirement already satisfied: packaging<24.0,>=23.2 in c:\\users\\shauw\\miniconda3\\envs\\cs585\\lib\\site-packages (from langchain-core<0.2,>=0.1.7->langchain-openai) (23.2)\n",
      "Requirement already satisfied: typing-extensions<5,>=4.7 in c:\\users\\shauw\\miniconda3\\envs\\cs585\\lib\\site-packages (from openai<2.0.0,>=1.6.1->langchain-openai) (4.9.0)\n",
      "Collecting distro<2,>=1.7.0\n",
      "  Downloading distro-1.9.0-py3-none-any.whl (20 kB)\n",
      "Collecting httpx<1,>=0.23.0\n",
      "  Downloading httpx-0.26.0-py3-none-any.whl (75 kB)\n",
      "     ---------------------------------------- 75.9/75.9 kB 2.1 MB/s eta 0:00:00\n",
      "Requirement already satisfied: tqdm>4 in c:\\users\\shauw\\miniconda3\\envs\\cs585\\lib\\site-packages (from openai<2.0.0,>=1.6.1->langchain-openai) (4.66.1)\n",
      "Requirement already satisfied: sniffio in c:\\users\\shauw\\miniconda3\\envs\\cs585\\lib\\site-packages (from openai<2.0.0,>=1.6.1->langchain-openai) (1.3.0)\n",
      "Requirement already satisfied: regex>=2022.1.18 in c:\\users\\shauw\\miniconda3\\envs\\cs585\\lib\\site-packages (from tiktoken<0.6.0,>=0.5.2->langchain-openai) (2023.8.8)\n",
      "Requirement already satisfied: idna>=2.8 in c:\\users\\shauw\\miniconda3\\envs\\cs585\\lib\\site-packages (from anyio<5,>=3->langchain-core<0.2,>=0.1.7->langchain-openai) (3.4)\n",
      "Collecting httpcore==1.*\n",
      "  Downloading httpcore-1.0.2-py3-none-any.whl (76 kB)\n",
      "     ---------------------------------------- 76.9/76.9 kB 4.4 MB/s eta 0:00:00\n",
      "Requirement already satisfied: certifi in c:\\users\\shauw\\miniconda3\\envs\\cs585\\lib\\site-packages (from httpx<1,>=0.23.0->openai<2.0.0,>=1.6.1->langchain-openai) (2023.7.22)\n",
      "Collecting h11<0.15,>=0.13\n",
      "  Downloading h11-0.14.0-py3-none-any.whl (58 kB)\n",
      "     ---------------------------------------- 58.3/58.3 kB 3.0 MB/s eta 0:00:00\n",
      "Requirement already satisfied: jsonpointer>=1.9 in c:\\users\\shauw\\miniconda3\\envs\\cs585\\lib\\site-packages (from jsonpatch<2.0,>=1.33->langchain-core<0.2,>=0.1.7->langchain-openai) (2.3)\n",
      "Requirement already satisfied: pydantic-core==2.14.6 in c:\\users\\shauw\\miniconda3\\envs\\cs585\\lib\\site-packages (from pydantic<3,>=1->langchain-core<0.2,>=0.1.7->langchain-openai) (2.14.6)\n",
      "Requirement already satisfied: annotated-types>=0.4.0 in c:\\users\\shauw\\miniconda3\\envs\\cs585\\lib\\site-packages (from pydantic<3,>=1->langchain-core<0.2,>=0.1.7->langchain-openai) (0.6.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\shauw\\miniconda3\\envs\\cs585\\lib\\site-packages (from requests<3,>=2->langchain-core<0.2,>=0.1.7->langchain-openai) (3.3.2)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\shauw\\miniconda3\\envs\\cs585\\lib\\site-packages (from requests<3,>=2->langchain-core<0.2,>=0.1.7->langchain-openai) (2.1.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\shauw\\miniconda3\\envs\\cs585\\lib\\site-packages (from tqdm>4->openai<2.0.0,>=1.6.1->langchain-openai) (0.4.6)\n",
      "Installing collected packages: h11, distro, tiktoken, httpcore, httpx, openai, langchain-openai\n",
      "Successfully installed distro-1.9.0 h11-0.14.0 httpcore-1.0.2 httpx-0.26.0 langchain-openai-0.0.2.post1 openai-1.7.2 tiktoken-0.5.2\n"
     ]
    }
   ],
   "source": [
    "!pip install langchain-openai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting langchainhub\n",
      "  Downloading langchainhub-0.1.14-py3-none-any.whl (3.4 kB)\n",
      "Requirement already satisfied: requests<3,>=2 in c:\\users\\shauw\\miniconda3\\envs\\cs585\\lib\\site-packages (from langchainhub) (2.31.0)\n",
      "Collecting types-requests<3.0.0.0,>=2.31.0.2\n",
      "  Downloading types_requests-2.31.0.20240106-py3-none-any.whl (14 kB)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\shauw\\miniconda3\\envs\\cs585\\lib\\site-packages (from requests<3,>=2->langchainhub) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\shauw\\miniconda3\\envs\\cs585\\lib\\site-packages (from requests<3,>=2->langchainhub) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\shauw\\miniconda3\\envs\\cs585\\lib\\site-packages (from requests<3,>=2->langchainhub) (2.1.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\shauw\\miniconda3\\envs\\cs585\\lib\\site-packages (from requests<3,>=2->langchainhub) (2023.7.22)\n",
      "Installing collected packages: types-requests, langchainhub\n",
      "Successfully installed langchainhub-0.1.14 types-requests-2.31.0.20240106\n"
     ]
    }
   ],
   "source": [
    "!pip install langchainhub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting en-core-web-lg==3.7.1\n",
      "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_lg-3.7.1/en_core_web_lg-3.7.1-py3-none-any.whl (587.7 MB)\n",
      "     -------------------------------------- 587.7/587.7 MB 4.6 MB/s eta 0:00:00\n",
      "Requirement already satisfied: spacy<3.8.0,>=3.7.2 in c:\\users\\shauw\\miniconda3\\envs\\cs585\\lib\\site-packages (from en-core-web-lg==3.7.1) (3.7.2)\n",
      "Requirement already satisfied: thinc<8.3.0,>=8.1.8 in c:\\users\\shauw\\miniconda3\\envs\\cs585\\lib\\site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (8.2.2)\n",
      "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in c:\\users\\shauw\\miniconda3\\envs\\cs585\\lib\\site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (1.0.5)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in c:\\users\\shauw\\miniconda3\\envs\\cs585\\lib\\site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (2.31.0)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in c:\\users\\shauw\\miniconda3\\envs\\cs585\\lib\\site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (3.0.9)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\shauw\\miniconda3\\envs\\cs585\\lib\\site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (23.2)\n",
      "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in c:\\users\\shauw\\miniconda3\\envs\\cs585\\lib\\site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (2.4.8)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in c:\\users\\shauw\\miniconda3\\envs\\cs585\\lib\\site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (4.66.1)\n",
      "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in c:\\users\\shauw\\miniconda3\\envs\\cs585\\lib\\site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (2.5.3)\n",
      "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in c:\\users\\shauw\\miniconda3\\envs\\cs585\\lib\\site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (3.0.12)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\shauw\\miniconda3\\envs\\cs585\\lib\\site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (3.1.2)\n",
      "Requirement already satisfied: smart-open<7.0.0,>=5.2.1 in c:\\users\\shauw\\miniconda3\\envs\\cs585\\lib\\site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (6.4.0)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in c:\\users\\shauw\\miniconda3\\envs\\cs585\\lib\\site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (2.0.8)\n",
      "Requirement already satisfied: typer<0.10.0,>=0.3.0 in c:\\users\\shauw\\miniconda3\\envs\\cs585\\lib\\site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (0.9.0)\n",
      "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in c:\\users\\shauw\\miniconda3\\envs\\cs585\\lib\\site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (1.1.2)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in c:\\users\\shauw\\miniconda3\\envs\\cs585\\lib\\site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (1.0.10)\n",
      "Requirement already satisfied: setuptools in c:\\users\\shauw\\miniconda3\\envs\\cs585\\lib\\site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (65.6.3)\n",
      "Requirement already satisfied: numpy>=1.15.0 in c:\\users\\shauw\\miniconda3\\envs\\cs585\\lib\\site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (1.24.4)\n",
      "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in c:\\users\\shauw\\miniconda3\\envs\\cs585\\lib\\site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (2.0.10)\n",
      "Requirement already satisfied: weasel<0.4.0,>=0.1.0 in c:\\users\\shauw\\miniconda3\\envs\\cs585\\lib\\site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (0.3.4)\n",
      "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in c:\\users\\shauw\\miniconda3\\envs\\cs585\\lib\\site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (3.3.0)\n",
      "Requirement already satisfied: annotated-types>=0.4.0 in c:\\users\\shauw\\miniconda3\\envs\\cs585\\lib\\site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (0.6.0)\n",
      "Requirement already satisfied: pydantic-core==2.14.6 in c:\\users\\shauw\\miniconda3\\envs\\cs585\\lib\\site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (2.14.6)\n",
      "Requirement already satisfied: typing-extensions>=4.6.1 in c:\\users\\shauw\\miniconda3\\envs\\cs585\\lib\\site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (4.9.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\shauw\\miniconda3\\envs\\cs585\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (3.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\shauw\\miniconda3\\envs\\cs585\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (2023.7.22)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\shauw\\miniconda3\\envs\\cs585\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (3.3.2)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\shauw\\miniconda3\\envs\\cs585\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (2.1.0)\n",
      "Requirement already satisfied: blis<0.8.0,>=0.7.8 in c:\\users\\shauw\\miniconda3\\envs\\cs585\\lib\\site-packages (from thinc<8.3.0,>=8.1.8->spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (0.7.11)\n",
      "Requirement already satisfied: confection<1.0.0,>=0.0.1 in c:\\users\\shauw\\miniconda3\\envs\\cs585\\lib\\site-packages (from thinc<8.3.0,>=8.1.8->spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (0.1.4)\n",
      "Requirement already satisfied: colorama in c:\\users\\shauw\\miniconda3\\envs\\cs585\\lib\\site-packages (from tqdm<5.0.0,>=4.38.0->spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (0.4.6)\n",
      "Requirement already satisfied: click<9.0.0,>=7.1.1 in c:\\users\\shauw\\miniconda3\\envs\\cs585\\lib\\site-packages (from typer<0.10.0,>=0.3.0->spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (8.1.7)\n",
      "Requirement already satisfied: cloudpathlib<0.17.0,>=0.7.0 in c:\\users\\shauw\\miniconda3\\envs\\cs585\\lib\\site-packages (from weasel<0.4.0,>=0.1.0->spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (0.16.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\shauw\\miniconda3\\envs\\cs585\\lib\\site-packages (from jinja2->spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (2.1.2)\n",
      "Installing collected packages: en-core-web-lg\n",
      "Successfully installed en-core-web-lg-3.7.1\n",
      "\u001b[38;5;2m[+] Download and installation successful\u001b[0m\n",
      "You can now load the package via spacy.load('en_core_web_lg')\n"
     ]
    }
   ],
   "source": [
    "# !pip install spacy\n",
    "!python -m spacy download en_core_web_lg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting datasets\n",
      "  Downloading datasets-2.16.1-py3-none-any.whl (507 kB)\n",
      "     -------------------------------------- 507.1/507.1 kB 7.9 MB/s eta 0:00:00\n",
      "Collecting dill<0.3.8,>=0.3.0\n",
      "  Downloading dill-0.3.7-py3-none-any.whl (115 kB)\n",
      "     -------------------------------------- 115.3/115.3 kB 7.0 MB/s eta 0:00:00\n",
      "Requirement already satisfied: packaging in c:\\users\\shauw\\miniconda3\\envs\\cs585\\lib\\site-packages (from datasets) (23.2)\n",
      "Collecting fsspec[http]<=2023.10.0,>=2023.1.0\n",
      "  Downloading fsspec-2023.10.0-py3-none-any.whl (166 kB)\n",
      "     ---------------------------------------- 166.4/166.4 kB ? eta 0:00:00\n",
      "Requirement already satisfied: pandas in c:\\users\\shauw\\miniconda3\\envs\\cs585\\lib\\site-packages (from datasets) (2.0.3)\n",
      "Requirement already satisfied: requests>=2.19.0 in c:\\users\\shauw\\miniconda3\\envs\\cs585\\lib\\site-packages (from datasets) (2.31.0)\n",
      "Collecting pyarrow>=8.0.0\n",
      "  Downloading pyarrow-14.0.2-cp38-cp38-win_amd64.whl (24.6 MB)\n",
      "     --------------------------------------- 24.6/24.6 MB 40.9 MB/s eta 0:00:00\n",
      "Collecting pyarrow-hotfix\n",
      "  Downloading pyarrow_hotfix-0.6-py3-none-any.whl (7.9 kB)\n",
      "Requirement already satisfied: tqdm>=4.62.1 in c:\\users\\shauw\\miniconda3\\envs\\cs585\\lib\\site-packages (from datasets) (4.66.1)\n",
      "Collecting multiprocess\n",
      "  Downloading multiprocess-0.70.15-py38-none-any.whl (132 kB)\n",
      "     -------------------------------------- 132.6/132.6 kB 7.6 MB/s eta 0:00:00\n",
      "Collecting huggingface-hub>=0.19.4\n",
      "  Downloading huggingface_hub-0.20.2-py3-none-any.whl (330 kB)\n",
      "     ------------------------------------- 330.3/330.3 kB 10.3 MB/s eta 0:00:00\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\shauw\\miniconda3\\envs\\cs585\\lib\\site-packages (from datasets) (6.0)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\shauw\\miniconda3\\envs\\cs585\\lib\\site-packages (from datasets) (1.24.4)\n",
      "Collecting filelock\n",
      "  Downloading filelock-3.13.1-py3-none-any.whl (11 kB)\n",
      "Collecting xxhash\n",
      "  Downloading xxhash-3.4.1-cp38-cp38-win_amd64.whl (29 kB)\n",
      "Requirement already satisfied: aiohttp in c:\\users\\shauw\\miniconda3\\envs\\cs585\\lib\\site-packages (from datasets) (3.9.1)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in c:\\users\\shauw\\miniconda3\\envs\\cs585\\lib\\site-packages (from aiohttp->datasets) (1.4.1)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0 in c:\\users\\shauw\\miniconda3\\envs\\cs585\\lib\\site-packages (from aiohttp->datasets) (4.0.3)\n",
      "Requirement already satisfied: attrs>=17.3.0 in c:\\users\\shauw\\miniconda3\\envs\\cs585\\lib\\site-packages (from aiohttp->datasets) (22.2.0)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in c:\\users\\shauw\\miniconda3\\envs\\cs585\\lib\\site-packages (from aiohttp->datasets) (1.9.4)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in c:\\users\\shauw\\miniconda3\\envs\\cs585\\lib\\site-packages (from aiohttp->datasets) (6.0.4)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in c:\\users\\shauw\\miniconda3\\envs\\cs585\\lib\\site-packages (from aiohttp->datasets) (1.3.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\shauw\\miniconda3\\envs\\cs585\\lib\\site-packages (from huggingface-hub>=0.19.4->datasets) (4.9.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\shauw\\miniconda3\\envs\\cs585\\lib\\site-packages (from requests>=2.19.0->datasets) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\shauw\\miniconda3\\envs\\cs585\\lib\\site-packages (from requests>=2.19.0->datasets) (2.1.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\shauw\\miniconda3\\envs\\cs585\\lib\\site-packages (from requests>=2.19.0->datasets) (2023.7.22)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\shauw\\miniconda3\\envs\\cs585\\lib\\site-packages (from requests>=2.19.0->datasets) (3.3.2)\n",
      "Requirement already satisfied: colorama in c:\\users\\shauw\\miniconda3\\envs\\cs585\\lib\\site-packages (from tqdm>=4.62.1->datasets) (0.4.6)\n",
      "Requirement already satisfied: tzdata>=2022.1 in c:\\users\\shauw\\miniconda3\\envs\\cs585\\lib\\site-packages (from pandas->datasets) (2023.3)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\shauw\\miniconda3\\envs\\cs585\\lib\\site-packages (from pandas->datasets) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\shauw\\miniconda3\\envs\\cs585\\lib\\site-packages (from pandas->datasets) (2023.3.post1)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\shauw\\miniconda3\\envs\\cs585\\lib\\site-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.16.0)\n",
      "Installing collected packages: xxhash, pyarrow-hotfix, pyarrow, fsspec, filelock, dill, multiprocess, huggingface-hub, datasets\n",
      "Successfully installed datasets-2.16.1 dill-0.3.7 filelock-3.13.1 fsspec-2023.10.0 huggingface-hub-0.20.2 multiprocess-0.70.15 pyarrow-14.0.2 pyarrow-hotfix-0.6 xxhash-3.4.1\n"
     ]
    }
   ],
   "source": [
    "!pip install datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\shauw\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\shauw\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "import spacy\n",
    "\n",
    "nlp = spacy.load('en_core_web_lg')\n",
    "\n",
    "# Ensure you have the necessary NLTK data\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "\n",
    "from langchain.agents import Tool\n",
    "from datasets import load_dataset\n",
    "\n",
    "# dataset = load_dataset(\"NebulaByte/E-Commerce_Customer_Support_Conversations\")\n",
    "\n",
    "class CustomDatasetTool(Tool):\n",
    "    def __init__(self):\n",
    "        self.dataset = load_dataset(\"NebulaByte/E-Commerce_Customer_Support_Conversations\")\n",
    "    \n",
    "    def extract_keywords(self, text): # use spaCy instead here, using cosine similarity\n",
    "        stop_words = set(stopwords.words('english'))\n",
    "        words = word_tokenize(text)\n",
    "        keywords = [word for word in words if word.isalpha() and word not in stop_words]\n",
    "        \n",
    "        return keywords\n",
    "    \n",
    "    def rank_results(self, results): # based on cosine similarity rank em\n",
    "        results.sort(key=lambda x: x[1])\n",
    "        return results\n",
    "    \n",
    "    def matches_query(self, record, keywords):\n",
    "        # use matching keywords with the record\n",
    "        # get spacy to get cosine similiarity between keywords and questions\n",
    "        \n",
    "        sentence1 = nlp(unicode(record))\n",
    "        sentence2 = nlp(unicode(keywords))\n",
    "        return sentence1.similarity(sentence2)\n",
    "    \n",
    "    def format_results(self, ranked_results): # put asnwer here not question\n",
    "        return ranked_results[0][0]\n",
    "    \n",
    "    def search(self, query):\n",
    "        # Extract keywords or intent from the query\n",
    "        keywords = extract_keywords(query)\n",
    "\n",
    "        # Search the dataset\n",
    "        results = []\n",
    "        for record in self.dataset:\n",
    "            match_score =  matches_query(record['conversation'], keywords)\n",
    "            if match_score > 0.5:  # Implement this function based on your dataset structure\n",
    "                results.append([record,  match_score])\n",
    "\n",
    "        # Rank the results - this can be as simple or complex as you need\n",
    "        ranked_results = rank_results(results)    \n",
    "\n",
    "        # Return the top result or top few results\n",
    "        return format_results(ranked_results)  # Implement formatting based on your desired output\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3m I should search for LangChain on tavily_search_results_json\n",
      "Action: tavily_search_results_json\n",
      "Action Input: \"LangChain\"\u001b[0m\u001b[36;1m\u001b[1;3m[{'url': 'https://medium.com/@okanyenigun/langchain-in-chains-1-a-closer-look-a6bbaa246ccb', 'content': 'Langchain  LangChain is an open-source framework designed to simplify the creation of applications using large language models  Langchain provides various components that make it easier to integrate and manage models in different application  LangChain is a framework for developing applications powered by language models. It enables applications that:This is the first part of a series about LangChain. I will introduce its core concepts. In each subsequent post, we will expand our understanding of the framework. LangChain is an open-source…'}]\u001b[0m\u001b[32;1m\u001b[1;3mI need more information about LangChain\n",
      "Action: tavily_search_results_json\n",
      "Action Input: \"LangChain framework\"\u001b[0m\u001b[36;1m\u001b[1;3m[{'url': 'https://blog.stackademic.com/langchain-in-chains-1-a-closer-look-a6bbaa246ccb', 'content': 'Langchain  LangChain is an open-source framework designed to simplify the creation of applications using large language models  Langchain provides various components that make it easier to integrate and manage models in different application  LangChain is a framework for developing applications powered by language models. It enables applications that:6 min read · 1 day ago -- This is the first part of a series about LangChain. I will introduce its core concepts. In each subsequent post, we will expand our understanding of the framework. LangChain is an open-source framework designed to simplify the creation of applications using large language models (LLMs).'}]\u001b[0m\u001b[32;1m\u001b[1;3mI should read the blog post to get a better understanding of LangChain\n",
      "Action: tavily_search_results_json\n",
      "Action Input: \"LangChain blog post\"\u001b[0m\u001b[36;1m\u001b[1;3m[{'url': 'https://blog.langchain.dev/langchain-v0-1-0/', 'content': \"LangChain v0.1.0  LangChain 0.2  LangChain has been around for a little over a year and has changed a lot as it’s grown to become the default framework  However, starting today with the release of langchain 0.1.0, all future releases will follow a new versioning standard.10 min read Jan 8, 2024 Today we're excited to announce the release of langchain 0.1.0, our first stable version. It is fully backwards compatible, comes in both Python and JavaScript, and comes with improved focus through both functionality and documentation.\"}]\u001b[0m\u001b[32;1m\u001b[1;3mI now know the final answer\n",
      "Final Answer: LangChain is an open-source framework designed to simplify the creation of applications using large language models. It enables applications that can process and understand natural language. It has been around for over a year and recently released its first stable version, langchain 0.1.0, which is fully backwards compatible and available in both Python and JavaScript.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'input': 'what is LangChain?',\n",
       " 'output': 'LangChain is an open-source framework designed to simplify the creation of applications using large language models. It enables applications that can process and understand natural language. It has been around for over a year and recently released its first stable version, langchain 0.1.0, which is fully backwards compatible and available in both Python and JavaScript.'}"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain import hub\n",
    "from langchain.agents import AgentExecutor, create_react_agent\n",
    "from langchain_community.tools.tavily_search import TavilySearchResults\n",
    "from langchain_openai import OpenAI\n",
    "\n",
    "\n",
    "import os\n",
    "os.environ['TAVILY_API_KEY'] = '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'\n",
    "os.environ['OPENAI_API_KEY'] = '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'\n",
    "tools = [TavilySearchResults(max_results=1)]\n",
    "\n",
    "# tavily_api_key = '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'\n",
    "# tools = [TavilySearchResults(tavily_api_key=tavily_api_key, max_results=1)]\n",
    "\n",
    "\n",
    "# Get the prompt to use - you can modify this!\n",
    "prompt = hub.pull(\"hwchase17/react\")\n",
    "\n",
    "# Choose the LLM to use\n",
    "llm = OpenAI()\n",
    "\n",
    "# Construct the ReAct agent\n",
    "agent = create_react_agent(llm, tools, prompt)\n",
    "\n",
    "# Create an agent executor by passing in the agent and tools\n",
    "agent_executor = AgentExecutor(agent=agent, tools=tools, verbose=True)\n",
    "\n",
    "agent_executor.invoke({\"input\": \"what is LangChain?\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# dataset = load_dataset(\"NebulaByte/E-Commerce_Customer_Support_Conversations\")\n",
    "# parsed_data = []\n",
    "# for record in dataset['train']:\n",
    "#     parsed_data.append({\n",
    "#         'question': record['conversation']\n",
    "#     }) \n",
    "# parsed_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#############################################################################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\shauw\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\shauw\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "import spacy\n",
    "\n",
    "nlp = spacy.load('en_core_web_lg')\n",
    "\n",
    "# Ensure you have the necessary NLTK data\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "\n",
    "from langchain.agents import Tool\n",
    "from datasets import load_dataset\n",
    "\n",
    "from langchain.agents import Tool\n",
    "from langchain.tools import BaseTool\n",
    "\n",
    "\n",
    "class ReplyGetter(BaseTool):\n",
    "    name = \"Get Response for User Query\"\n",
    "    description = \"Used to get the service bot to respond to the user query\"\n",
    "        \n",
    "    def load_dataset(self):\n",
    "        # Load the specific Hugging Face dataset\n",
    "        dataset = load_dataset(\"NebulaByte/E-Commerce_Customer_Support_Conversations\")\n",
    "        parsed_data = []\n",
    "        for record in dataset['train']:\n",
    "            parsed_data.append({\n",
    "                'question': record['conversation'] \n",
    "            })\n",
    "        return parsed_data\n",
    "\n",
    "#     def load_dataset(self, dataset: str):\n",
    "#         dataset = load_dataset(\"NebulaByte/E-Commerce_Customer_Support_Conversations\")\n",
    "#         print(dataset)\n",
    "#         return dataset\n",
    "   \n",
    "    def extract_keywords(self, text: str): # use spaCy instead here, using cosine similarity\n",
    "        stop_words = set(stopwords.words('english'))\n",
    "        words = word_tokenize(text)\n",
    "        keywords = [word for word in words if word.isalpha() and word not in stop_words]\n",
    "        \n",
    "        return str(keywords)\n",
    "    \n",
    "    def rank_results(self, results:list): # based on cosine similarity rank em\n",
    "        results.sort(key=lambda x: x[1])\n",
    "        return results\n",
    "    \n",
    "    def matches_query(self, record:str, keywords:str):\n",
    "        # use matching keywords with the record\n",
    "        # get spacy to get cosine similiarity between keywords and questions\n",
    "        \n",
    "        sentence1 = nlp(record)\n",
    "        sentence2 = nlp(keywords)\n",
    "        return sentence1.similarity(sentence2)\n",
    "    \n",
    "    def format_results(self, ranked_results:list): # put asnwer here not question\n",
    "        return ranked_results[0][0]\n",
    "    \n",
    "    def _run(self, query:str):\n",
    "        # Extract keywords or intent from the query\n",
    "        keywords = self.extract_keywords(query)\n",
    "\n",
    "        # get the dataset loaded\n",
    "        dataset = self.load_dataset()\n",
    "        \n",
    "        # Search the dataset\n",
    "        results = []\n",
    "        \n",
    "        for record in dataset:\n",
    "            match_score =  self.matches_query(record['question'], keywords)\n",
    "            if match_score > 0.1:  # Implement this function based on your dataset structure\n",
    "                results.append([record,  match_score])\n",
    "\n",
    "        # Rank the results - this can be as simple or complex as you need\n",
    "        ranked_results = self.rank_results(results)    \n",
    "\n",
    "        # Return the top result or top few results\n",
    "        return self.format_results(ranked_results)  # Implement formatting based on your desired output\n",
    "\n",
    "    def _arun(self, webpage: str):\n",
    "        raise NotImplementedError(\"This tool does not support async\")\n",
    "\n",
    "reply_getter = ReplyGetter()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3m I should use Get Response for User Query to check on the delivery status of the laptop\n",
      "Action: Get Response for User Query\n",
      "Action Input: \"What is the status of my laptop delivery?\"\u001b[0m\u001b[36;1m\u001b[1;3m{'question': \"Agent: Hello, thank you for contacting BrownBox customer support. My name is Sarah. How may I assist you today?\\n\\nCustomer: Hi Sarah, I am trying to order a shirt, but I can't seem to get it shipped to my location. Also, I need faster delivery options.\\n\\nAgent: I'm sorry to hear that you're having trouble with your order. May I have your name and email address, please?\\n\\nCustomer: My name is Jane Smith, and my email address is janesmith@gmail.com.\\n\\nAgent: Thank you, Jane. I'm sorry to hear that you're experiencing this issue. May I know the item you're trying to order and your location?\\n\\nCustomer: I'm trying to order a shirt, and my location is in New York.\\n\\nAgent: I see. I'm sorry to inform you that we don't currently ship that particular shirt to New York. However, we do have similar products that we can recommend to you. Would you like me to help you find a similar product that we can ship to your location?\\n\\nCustomer: No, I don't want a similar product. I want that specific shirt. Also, I need faster delivery options. I need it urgently.\\n\\nAgent: I understand your concern, Jane. I'm sorry to inform you that we don't have faster delivery options available for that particular shirt. However, we do have other products that we can recommend to you, which we can deliver to you faster. Would you like me to help you find a similar product that we can ship to you faster?\\n\\nCustomer: No, I don't want a similar product. This is unacceptable. I need that shirt, and I need it urgently. I'm very disappointed with your service.\\n\\nAgent: I'm sorry to hear that you're disappointed, Jane. I understand how frustrating this can be for you. However, I assure you that we're doing everything we can to improve our services. In the meantime, can I suggest that you check our website regularly for updates on the availability of the shirt you're looking for?\\n\\nCustomer: Okay, I'll do that. But I'm still not happy about this.\\n\\nAgent: I understand, Jane. Is there anything else I can help you with?\\n\\nCustomer: No, that's all.\\n\\nAgent: Thank you for contacting BrownBox customer support. Have a good day.\"}\u001b[0m\u001b[32;1m\u001b[1;3m I should analyze the response from the customer support agent to find out the status of my laptop delivery\n",
      "Action: Get Response for User Query\n",
      "Action Input: \"What is the status of my laptop delivery?\"\u001b[0m"
     ]
    }
   ],
   "source": [
    "from langchain import hub\n",
    "from langchain.agents import AgentExecutor, create_react_agent\n",
    "# from langchain_community.tools.tavily_search import TavilySearchResults\n",
    "from langchain_openai import OpenAI\n",
    "import os\n",
    "\n",
    "\n",
    "os.environ['OPENAI_API_KEY'] = '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'\n",
    "\n",
    "tools = [reply_getter]\n",
    "\n",
    "\n",
    "# Get the prompt to use - you can modify this!\n",
    "prompt = hub.pull(\"hwchase17/react\")\n",
    "\n",
    "# Choose the LLM to use\n",
    "llm = OpenAI()\n",
    "\n",
    "# Construct the ReAct agent\n",
    "agent = create_react_agent(llm, tools, prompt)\n",
    "\n",
    "# Create an agent executor by passing in the agent and tools\n",
    "agent_executor = AgentExecutor(agent=agent, tools=tools, verbose=True)\n",
    "\n",
    "agent_executor.invoke({\"input\": \"My laptop hasn't been delivered?\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
